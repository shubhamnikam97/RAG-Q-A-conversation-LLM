# ğŸ“š Conversational RAG Chatbot with PDF Upload + Chat History  
A Streamlit-based **Retrieval-Augmented Generation (RAG)** chatbot that allows users to upload PDFs and interact with their content conversationally.  
The system maintains full **chat history**, reformulates contextual questions, and retrieves relevant document chunks using embeddings and a vector database.

Built using **LangChain**, **Groq LLM**, **HuggingFace Embeddings**, and **ChromaDB**.

---

## ğŸš€ Features

- **Conversational PDF Q&A**  
  Ask natural-language questions about your PDFs using AI.

- **Chat History Memory**  
  Each session stores its own conversation context using LangChain's message history.

- **RAG Pipeline**  
  Uses:
  - Document splitting  
  - Embeddings  
  - Vector storage  
  - Context-aware question rewriting  
  - Document retrieval  
  - LLM answer generation  

- **Multiple PDF Support**  
  Upload one or more PDFs per session.

- **Local Embeddings (Fast & Free)**  
  Uses `all-MiniLM-L6-v2` via HuggingFace to create embeddings locally.

- **Groq LLM Integration**  
  Powered by the ultra-fast `llama-3.1-8b-instant` model.

- **History-Aware Retrieval**  
  The system automatically rephrases questions using chat context before querying the vector store.

---

## ğŸ§  How the System Works

This RAG application follows a multi-step pipeline:

### **1ï¸âƒ£ PDF Upload & Processing**
- PDFs are uploaded through Streamlit
- Processed using **PyPDFium2Loader**
- Split into overlapping chunks via **RecursiveCharacterTextSplitter**

### **2ï¸âƒ£ Embeddings + Vector DB**
- Chunks are converted into dense embeddings using:
- Stored inside a **Chroma** vector database

### **3ï¸âƒ£ History-Aware Retrieval**
Before retrieving, the system asks the LLM to rewrite the userâ€™s question using prior messages:

> *"Given chat history + question, generate a standalone version of this question."*

This improves retrieval accuracy.

### **4ï¸âƒ£ RAG Chain**
Retrieved documents are passed into a prompt + Groq LLM to generate final concise answers.

### **5ï¸âƒ£ Chat Memory**
Each session ID maintains persistent chat history using:


This allows long conversations about PDFs.

---

## ğŸ“¦ Tech Stack

- **Python 3.12+**
- **Streamlit**
- **LangChain Classic**
- **LangChain Chroma**
- **HuggingFace Embeddings**
- **Chroma Vector DB**
- **Groq LLM API**
- **PyPDFium2 + PyPDFLoader**

---

## ğŸ“ Project Structure
```
.
â”œâ”€â”€ app.py # Main RAG Chatbot application
â”œâ”€â”€ requirements.txt # Dependencies
â””â”€â”€ README.md # Documentation
```
---

## ğŸ”§ Installation & Setup

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/yourusername/your-repo.git
cd your-repo
```

2ï¸âƒ£ Create & activate a virtual environment
```bash
python -m venv venv
source venv/bin/activate       # macOS/Linux
venv\Scripts\activate          # Windows
```

3ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```

4ï¸âƒ£ Add HuggingFace & Groq API keys
```bash
HF_TOKEN=your_hf_token
GROQ_API_KEY=your_groq_api_key
```

â–¶ï¸ Run the Application
```bash
streamlit run "file_path/app.py"
```

ğŸ–¥ Usage Instructions
âœ” Step 1 â€” Enter your Groq API key

Using sidebar or .env.

âœ” Step 2 â€” Upload PDFs

Supports multiple documents.

âœ” Step 3 â€” Enter a session ID

Example: my_research_session

âœ” Step 4 â€” Ask questions

Examples:

â€œSummarize section 2.â€

â€œWhat did the author say about neural networks?â€

â€œHow does this compare to chapter 3?â€

âœ” Step 5 â€” View chat history

The app displays real-time session memory.
